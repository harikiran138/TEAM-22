from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional
from ai_engine.llm import get_llm_provider
from ai_engine.rag import rag_engine

router = APIRouter()
llm = get_llm_provider()

class CourseGenerationRequest(BaseModel):
    topic: str
    level: str = "Beginning"
    modules: int = 4

class CourseGenerationResponse(BaseModel):
    title: str
    description: str
    outline: List[str]

class TutorChatRequest(BaseModel):
    message: str
    user_id: str = "guest"
    context_filters: Optional[dict] = None

class IngestRequest(BaseModel):
    text: str
    metadata: Optional[dict] = None

@router.post("/generate-course", response_model=CourseGenerationResponse)
async def generate_course(request: CourseGenerationRequest):
    """
    Generate a course outline using local Ollama instance.
    """
    system_prompt = "You are an expert curriculum designer. Create a structured course outline."
    user_prompt = f"""
    Create a {request.level} level course about "{request.topic}".
    The course should have exactly {request.modules} modules.
    
    Return the response in strictly valid JSON format with the following structure:
    {{
        "title": "Course Title",
        "description": "Brief summary of the course",
        "outline": [
            "Module 1: Title - Brief Description",
            "Module 2: Title - Brief Description",
            ...
        ]
    }}
    Do not include any explanation, only the JSON.
    """
    
    try:
        raw_response = llm.generate(user_prompt, system_prompt)
        # Simple cleanup
        clean_response = raw_response.replace("```json", "").replace("```", "").strip()
        import json
        data = json.loads(clean_response)
        
        return CourseGenerationResponse(
            title=data.get("title", f"Course on {request.topic}"),
            description=data.get("description", "Generated by AI"),
            outline=data.get("outline", [])
        )
    except Exception as e:
        print(f"Generation Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/tutor/chat")
async def tutor_chat(request: TutorChatRequest):
    """
    Chat with the AI Tutor using RAG context and Pathway personalization.
    """
    try:
        # 1. Retrieve Context
        relevant_docs = rag_engine.query(request.message)
        context_str = "\n\n".join(relevant_docs)
        
        # 2. Retrieve Learner State & Pathway Recommendation
        # Initialize store if not global (or import and use singular instance)
        from learner_profile.store.state import StateStore
        store = StateStore()
        state = store.get_state(request.user_id)
        
        behavior = state.get("behavior_label", "neutral")
        # Get recommendation (pass empty graph for now/default)
        recommendation = pathway_agent.recommend_next_node(state, {})
        
        # 3. Construct Prompt with Personalization
        system_prompt = (
            "You are a helpful AI Tutor. Use the provided Context to answer the user's question.\n"
            f"Adapt your response to the learner's profile:\n"
            f"- Behavior: {behavior} (If 'frustrated', be encouraging. If 'focused', be concise).\n"
            f"- Pathway Recommendation: {recommendation} (If 'review', emphasize basics. If 'advance', challenge them).\n"
            "If the answer is not in the context, use your general knowledge but mention that it's outside the course material."
        )
        
        user_prompt = f"""
        Context:
        {context_str}
        
        Question: {request.message}
        """
        
        # 4. Generate Answer
        response = llm.generate(user_prompt, system_prompt)
        return {"response": response, "context_used": relevant_docs, "personalization": {"behavior": behavior, "recommendation": recommendation}}
        
    except Exception as e:
        print(f"Tutor Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/tutor/ingest")
async def ingest_content(request: IngestRequest):
    """
    Ingest text content into the RAG vector store.
    """
    try:
        rag_engine.ingest_text(request.text, request.metadata)
        return {"status": "success", "message": "Content ingested successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# --- Pathway Agent Integration ---
from ai_engine.swarm.pathway import PathwayAgent
from learner_profile.models.behavior import BehaviorModel

pathway_agent = PathwayAgent()
behavior_model = BehaviorModel()

class PathwayRequest(BaseModel):
    learner_state: dict
    curriculum_graph: dict

class BehaviorRequest(BaseModel):
    session_data: dict

@router.post("/pathway/recommend")
async def recommend_pathway(request: PathwayRequest):
    """
    Get next node recommendation from Pathway Agent.
    """
    recommendation = pathway_agent.recommend_next_node(request.learner_state, request.curriculum_graph)
    return {"recommendation": recommendation}

@router.post("/profile/behavior")
async def classify_behavior(request: BehaviorRequest):
    """
    Classify learner behavior.
    """
    label = behavior_model.classify_behavior(request.session_data)
    score = behavior_model.calculate_engagement_score(request.session_data)
    return {"behavior": label, "engagement_score": score}

