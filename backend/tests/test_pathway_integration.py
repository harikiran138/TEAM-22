import sys
import os
import pytest
from fastapi.testclient import TestClient
from unittest.mock import MagicMock, patch

# Add backend to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# MOCK TORCH ONLY to fix import issues, but let others load
sys.modules["torch"] = MagicMock()
sys.modules["torch.nn"] = MagicMock()

from app.main import app
from ai_engine.swarm.pathway import PathwayAgent

# Patch the engine method to return a float instead of a Mock (because torch is mocked)
@pytest.fixture(autouse=True)
def mock_inference_output():
    with patch("ai_engine.pathway.inference_engine.PathwayInferenceEngine.predict_mastery", return_value=0.8):
        yield



from ai_engine.swarm.pathway import PathwayAgent
from learner_profile.models.behavior import BehaviorModel
from learner_profile.engine import LearnerProfileEngine

# --- WHITE BOX TESTS ---
# Testing internal logic of classes

def test_behavior_model_classification():
    """Only tests the logic inside BehaviorModel (White Box)"""
    model = BehaviorModel()
    
    # Test specific internal logic branches
    focused_data = {"interactions": [1]*10, "avg_response_time": 5}
    distracted_data = {"interactions": [1], "avg_response_time": 15}
    
    assert model.classify_behavior(focused_data) == "focused"
    assert model.classify_behavior(distracted_data) == "distracted"

def test_pathway_agent_mock_inference():
    """Tests PathwayAgent logic, mocking the inference engine if complex"""
    agent = PathwayAgent()
    
    # We can inspect the internal engine state if needed for white box
    assert agent.engine is not None
    
    # Test decision logic
    # Mastery > 0.7 -> advance
    # We can't easily force the model prediction without mocking, 
    # but we can test the flow if we controlled the wrapper.
    # For now, we test that it runs without error and returns valid string
    res = agent.recommend_next_node({"skill_sequence": [1, 2], "correct_sequence": [1, 1]}, {})
    assert isinstance(res, str)
    assert res in ["advance_next_topic", "review_prerequisite", "practice_current_topic"]

def test_profile_engine_state_update():
    """Tests that the engine correctly updates the store"""
    engine = LearnerProfileEngine()
    user_id = "test_user_wb"
    
    # Initial state
    initial = engine.get_profile(user_id)
    assert initial["behavior_label"] == "neutral"
    
    # Update
    engine.update_state(user_id, {"behavior": "focused"})
    updated = engine.get_profile(user_id)
    assert updated["behavior_label"] == "focused"

# --- BLACK BOX TESTS ---
# Testing via public API endpoints

client = TestClient(app)

def test_api_recommend_pathway():
    """Black box test: Send input, get output, ignore internals"""
    payload = {
        "learner_state": {
            "skill_sequence": [1, 2, 3],
            "correct_sequence": [1, 0, 1]
        },
        "curriculum_graph": {}
    }
    response = client.post("/api/pathway/recommend", json=payload)
    assert response.status_code == 200
    data = response.json()
    assert "recommendation" in data
    assert isinstance(data["recommendation"], str)

def test_api_classify_behavior():
    """Black box test: Send session data, get classification"""
    payload = {
        "session_data": {
            "interactions": [1, 2, 3, 4, 5, 6, 7, 8, 9], # Should be enough for focused
            "avg_response_time": 2.0
        }
    }
    response = client.post("/api/profile/behavior", json=payload)
    assert response.status_code == 200
    data = response.json()
    assert "behavior" in data
    assert "engagement_score" in data
    # We don't care HOW it calculated it, just that it did
    assert data["behavior"] in ["focused", "distracted", "frustrated", "neutral", "idle"]
